<!DOCTYPE html>
<html lang="en" class="font-poppins">
  <head>
    <title>Raster Vision</title>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <script src="globals.js"></script>
    <script src="index.js"></script>
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.6.0/jquery.min.js"></script>
    <script src="https://unpkg.com/typewriter-effect@latest/dist/core.js"></script>
    <link rel="preconnect" href="https://fonts.googleapis.com" />
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
    <link
      href="https://fonts.googleapis.com/css2?family=Poppins:wght@300&display=swap"
      rel="stylesheet"
    />
    <link
      href="https://fonts.googleapis.com/css2?family=Poppins:wght@600&display=swap"
      rel="stylesheet"
    />
    <link rel="stylesheet" href="dist/output.css" />
    <link rel="shortcut icon" type="image/x-icon" href="images/favicon.ico?" />
  </head>

  <body class="overflow-x-hidden">
    <nav
      id="header"
      class="sticky sm:hidden top-0 bg-background-off-white h-16 z-50"
    >
      <div
        class="flex flex-row h-16 items-center px-8 sm:px-2 md:px-8 justify-between"
      >
        <div class="basis-1/2 sm:basis-1/3">
          <img src="images/rv-logo.svg" class="relative h-8" />
        </div>
        <div class="flex flex-row items-center">
          <a href="https://github.com/azavea/raster-vision/discussions">
            <div
              class="hidden sm:flex text-black text-opacity-60 font-semibold hover:underline mr-1.5 md:mr-6"
            >
              Github Discussions
            </div>
          </a>
          <a href="https://docs.rastervision.io/">
            <div
              class="hidden sm:flex text-black text-opacity-60 font-semibold hover:underline mr-1.5 md:mr-6"
            >
              Documentation
            </div>
          </a>
          <a href="https://github.com/azavea/raster-vision">
            <button class="call-to-action-btn">Get started</button>
          </a>
        </div>
      </div>
    </nav>
    <section class="bg-background-off-white">
      <div class="flex flex-col sm:block">
        <div
          class="relative top-0 w-full h-80 sm:absolute sm:right-0 sm:w-hero sm:max-w-hero sm:h-hero sm:min-h-hero"
        >
          <img
            src="images/hero-trafficdensity.png"
            alt="Traffic on a highway interchange"
            class="hero-image primary"
            id="hero-img-0"
          />
          <img
            src="images/hero-crophealth2.png"
            alt="Aerial view of fields of crops"
            class="hero-image secondary hidden"
            id="hero-img-1"
          />
        </div>
        <div class="flex max-width-container w-full py-0 sm:min-h-hero">
          <div class="flex flex-col max-w-linked-buttons w-3/4 left-0">
            <div>
              <img
                src="images/rv-logo.svg"
                class="relative top-14 hidden sm:block max-h-6"
              />
            </div>
            <div class="sm:h-40"></div>
            <div class="max-w-header-text z-40 min-h-hero-text">
              <h2 class="text-lg sm:text-2xl">An open-source ML library for</h2>
              <p
                id="hero-text"
                class="display mt-2 text-3xl leading-tight sm:text-4xl"
              ></p>
            </div>
            <div class="h-10"></div>
            <div class="flex gap-3 top-8 sm:top-32">
              <div
                id="left-button"
                class="rounded-full bg-gray-50 h-hero-button w-hero-button cursor-pointer"
              >
                <img
                  src="images/icon-chevron-left.svg"
                  class="mx-auto my-hero-button-arrow"
                />
              </div>
              <div
                id="right-button"
                class="rounded-full bg-gray-50 h-hero-button w-hero-button cursor-pointer"
              >
                <img
                  src="images/icon-chevron-right.svg"
                  class="mx-auto my-hero-button-arrow"
                />
              </div>
            </div>
            <div class="h-20"></div>
            <div class="top-16 flex flex-wrap sm:top-52 gap-x-3.5 z-40">
              <a href="https://github.com/azavea/raster-vision">
                <div class="hero-external-buttons group">
                  <h1 class="group-hover:text-teal">Get started</h1>
                  <h4 class="opacity-60">GitHub Repo</h4>
                </div>
              </a>
              <a href="https://docs.rastervision.io/">
                <div class="hero-external-buttons group">
                  <h1 class="group-hover:text-teal">Explore</h1>
                  <h4 class="opacity-60">Documentation</h4>
                </div>
              </a>
              <a href="https://github.com/azavea/raster-vision/discussions">
                <div class="hero-external-buttons group">
                  <h1 class="group-hover:text-teal">Discuss</h1>
                  <h4 class="opacity-60">GitHub Discussions</h4>
                </div>
              </a>
            </div>
          </div>
        </div>
      </div>
    </section>
    <section>
      <div
        id="subHero"
        class="bg-gradient-to-r from-gradient-left to-gradient-right"
      >
        <div class="max-width-container">
          <div class="mt-14 sm:flex sm:flex-row sm:mt-0 gap-10">
            <div class="basis-1/2 my-auto">
              <h1 class="text-2xl text-white mb-6 md:text-3xl">
                Build intelligent <i>geospatial</i> applications with deep
                learning.
              </h1>
              <p class="medium-body text-white">
                Raster Vision is an open source library and framework that
                bridges the divide between the world of GIS and deep
                learning-based computer vision. It provides a configurable
                computer vision pipeline that works on
                <b>chip classification, semantic segmentation,</b> and
                <b>object detection.</b>
              </p>
            </div>
            <div class="basis-1/2 max-w-xl my-auto">
              <img
                src="images/img-rv-computervision-tasks.png"
                class="m-auto sm:m-0 pt-10"
              />
            </div>
          </div>
        </div>
      </div>
    </section>
    <section>
      <div class="bg-background-off-white">
        <div class="max-width-container text-center mt-10">
          <h4 class="text-lightGray">How it Works</h4>
          <h1>Introducing the Raster Vision pipeline</h1>
          <p class="small-body text-lightGray">
            Example is showing a semantic segmentation task
          </p>
          <div class="sm:flex sm:flex-row gap-10 my-5">
            <div class="sm:w-1/3 mb-16">
              <div class="sm:h-64 flex items-center justify-center">
                <img src="images/rv_anim_ss_input.gif" class="m-auto sm:m-0" />
              </div>
              <div
                class="sm:h-72 md:h-64 lg:h-44 flex flex-col justify-between mx-6 sm:mx-0"
              >
                <div>
                  <h3><span class="text-teal">01</span> Data input</h3>
                  <p class="medium-body my-10 sm:my-0">
                    Input set of images and training data, optionally with Areas
                    of Interest (AOIs) that describe where the images are
                    labeled.
                  </p>
                </div>
                <a href="https://groundwork.azavea.com/">
                  <button class="secondary-btn">
                    Start training with GroundWork
                  </button>
                </a>
              </div>
            </div>
            <div class="sm:w-1/3 mb-16">
              <div class="sm:h-64 flex items-center justify-center">
                <img src="images/img_rv-pipeline.png" class="m-auto sm:m-0" />
              </div>
              <div
                class="sm:h-72 md:h-64 lg:h-44 flex flex-col justify-between mx-6 sm:mx-0"
              >
                <div>
                  <h3>
                    <span class="text-teal">02</span> Raster Vision pipeline
                  </h3>
                  <p class="medium-body my-10 sm:my-0">
                    The computer vision pipeline configuration documentation is
                    easy to read, reuse, and maintain.
                  </p>
                </div>
                <a href="https://docs.rastervision.io/">
                  <button class="secondary-btn">Explore documentation</button>
                </a>
              </div>
            </div>
            <div class="sm:w-1/3 mb-16">
              <div class="sm:h-64 flex items-center justify-center">
                <img src="images/rv_anim_ss_output.gif" class="m-auto sm:m-0" />
              </div>
              <div
                class="sm:h-72 md:h-64 lg:h-44 flex flex-col justify-between mx-6 sm:mx-0"
              >
                <div>
                  <h3><span class="text-teal">03</span> Deployment</h3>
                  <p class="medium-body my-10 sm:my-0">
                    Model bundle is deployed in batch processes, live servers,
                    and other custom workflows.
                  </p>
                </div>
                <div></div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </section>
    <section>
      <div class="bg-white">
        <div class="max-width-container">
          <div
            class="flex flex-col-reverse sm:flex-row justify-between my-4 sm:my-20 gap-6"
          >
            <div class="basis-2/5 text-center sm:text-left">
              <h1>Scaling <i>beyond</i> geospatial tech</h1>
              <p class="large-body leading-8 my-7">
                Raster Vision is versatile and can seamlessly handle the
                idiosyncrasies of working with massive image datasets across a
                broad range of industries.
              </p>
              <a href="https://groundwork.azavea.com/"
                ><div class="secondary-btn w-64 text-center mx-auto sm:mx-0">
                  Build your custom pipeline
                </div></a
              >
            </div>
            <div class="display h-48 m-auto overflow-hidden lg:mr-28">
              <!-- add bars to highlight only middle word in rolodex -->
              <div class="rolodex-bars"></div>
              <div class="rolodex-bars mt-24 rotate-180"></div>
              <ul class="animate-rolodex text-center z-20">
                <li>Forestry</li>
                <li>Healthcare</li>
                <li>Academia</li>
                <li>Non-profit</li>
                <li>Forestry</li>
                <li>Healthcare</li>
                <li>Academia</li>
              </ul>
            </div>
          </div>
        </div>
      </div>
    </section>
    <section>
      <div class="bg-background-blue">
        <div class="max-width-container">
          <div class="flex justify-center py-4">
            <button
              id="spec-button"
              class="sm:hidden text-white border-gray-button border-opacity-10 border-1 py-2 px-10 font-semibold"
            >
              See more specs
            </button>
          </div>
          <div id="spec-section" class="hidden sm:block sm:mb-28">
            <h1 class="text-center text-white">Technical specifications</h1>
            <div class="sm:grid sm:grid-cols-2 md:grid-cols-3">
              <div class="tech-spec text-white">
                <img src="images/timeline-arrow.svg" class="h-7 w-7 pb-2" />
                <p class="font-semibold text-med">
                  Offers a complete training pipeline for three computer vision
                  tasks
                </p>
                <p>
                  Including chip classification, object detection, and semantic
                  segmentation.
                </p>
              </div>
              <div class="tech-spec text-white">
                <img
                  src="images/circle-location-arrow.svg"
                  class="h-7 w-7 pb-2"
                />
                <p class="font-semibold text-med">
                  Produces georeferenced outputs
                </p>
                <p>
                  Raster Vision produces predictions that are georeferenced and
                  ready for downstream analysis.
                </p>
              </div>
              <div class="tech-spec text-white">
                <img src="images/draw-square.svg" class="h-7 w-7 pb-2" />
                <p class="font-semibold text-med">
                  Restrict training chips to an area or interest (AOI)
                </p>
                <p>
                  Fully annotating a large pixel scene is costly and time
                  consuming. Raster Vision supports specifying an AOI in the
                  form of one or more polygons.
                </p>
              </div>
              <div class="tech-spec text-white">
                <img src="images/hat-cowboy.svg" class="h-7 w-7 pb-2" />
                <p class="font-semibold text-med">Facilitates data wrangling</p>
                <p>
                  Raster Vision can ingest both raster and vector data and
                  convert them to a form suitable for training.
                </p>
              </div>
              <div class="tech-spec text-white">
                <img src="images/send-back.svg" class="h-7 w-7 pb-2" />
                <p class="font-semibold text-med">
                  Supports multiband imagery (eg. RGBIR images, Sentinel 2
                  imagery)
                </p>
                <p>
                  While retaining the existing pre-trained weights, Raster
                  Vision modifies the first convolutional layer to accept
                  additional (or fewer) channels.
                </p>
              </div>
              <div class="tech-spec text-white">
                <img src="images/aperture.svg" class="h-7 w-7 pb-2" />
                <p class="font-semibold text-med">Offers data augmentation</p>
                <p>
                  Raster Vision supports customizable Albumentations transforms
                  to use during training.
                </p>
              </div>
              <div class="tech-spec text-white">
                <img src="images/diagram-sankey.svg" class="h-7 w-7 pb-2" />
                <p class="font-semibold text-med">Custom model specification</p>
                <p>
                  Utilize a wide variety of compatible models from local
                  directories using TorchHub.
                </p>
              </div>
              <div class="tech-spec text-white">
                <img src="images/flame.svg" class="h-7 w-7 pb-2" />
                <p class="font-semibold text-med">Based on PyTorch</p>
                <p>
                  Raster Vision implements deep learning functionality with
                  PyTorch (a popular deep learning library) and Torchvision.
                </p>
              </div>
              <div class="tech-spec text-white">
                <img src="images/cloud-word.svg" class="h-7 w-7 pb-2" />
                <p class="font-semibold text-med">
                  Supports running pipelines in the cloud
                </p>
                <p>
                  Raster Vision provides support for running pipelines using AWS
                  Batch.
                </p>
              </div>
              <div class="tech-spec text-white">
                <img src="images/table-cells-large.svg" class="h-7 w-7 pb-2" />
                <p class="font-semibold text-med">
                  Supports configurable chip reading
                </p>
                <p>
                  Geospatial images tend to be too large to feed directly into a
                  neural network and must first be broken up into smaller
                  “chips”.
                </p>
              </div>
            </div>
          </div>
          <div class="text-center mb-8">
            <h2 class="text-white">
              Built on popular technologies & open source libraries including
            </h2>
            <p class="large-body text-white pb-4">
              AWS (S3 and Batch), PyTorch, TorchVision, Albumentations,
              Rasterio, Shapely, Gdal and Numpy.
            </p>
            <div
              class="grid grid-cols-2 gap-2 sm:grid-cols-4 sm:gap-x-16 max-w-xl m-auto place-items-center"
            >
              <div class="h-28 w-32 flex justify-center items-center">
                <a href="https://aws.amazon.com/">
                  <img src="images/logo-aws.svg" />
                </a>
              </div>
              <div class="h-28 w-32 flex justify-center items-center">
                <a href="https://pytorch.org/">
                  <img src="images/logo-pytorch.svg" />
                </a>
              </div>
              <div class="h-28 w-32 flex justify-center items-center">
                <a href="https://albumentations.ai/">
                  <img src="images/logo-abulmentations.svg" />
                </a>
              </div>
              <div class="h-28 w-32 flex justify-center items-center">
                <a href="https://numpy.org/">
                  <img src="images/logo-numpy.svg" />
                </a>
              </div>
            </div>
          </div>
        </div>
      </div>
    </section>
    <section>
      <div class="bg-background-off-white">
        <div class="max-width-container">
          <div class="text-center my-14">
            <h4 class="text-lightGray">Why Use Raster Vision</h4>
            <h1 class="max-w-2xl mx-auto mb-8">
              Employ extraordinary data processing power to meet your goals.
            </h1>
            <div class="grid grid-cols-2 md:grid-cols-4 gap-x-3 gap-y-8">
              <div>
                <img class="rounded-lg" src="images/why-use-customizable.jpg" />
                <p class="why-use-headline"><strong>Customizable</strong></p>
                <p class="xsmall-body text-lightGray">
                  Utilize custom datasets, model architectures, and arbitrary
                  loss functions.
                </p>
              </div>
              <div>
                <img class="rounded-lg" src="images/why-use-repeatable.jpg" />
                <p class="why-use-headline">
                  <strong>Repeatable, configurable workflow</strong>
                </p>
                <p class="xsmall-body text-lightGray">
                  Create consistent and maintainable results.
                </p>
              </div>
              <div>
                <img class="rounded-lg" src="images/why-use-opensource.jpg" />
                <p class="why-use-headline"><strong>Open source</strong></p>
                <p class="xsmall-body text-lightGray">
                  Benefit from a constantly updated & supported framework
                  released under the open source Apache 2.0 license.
                </p>
              </div>
              <div>
                <img class="rounded-lg" src="images/why-use-geodatasets.jpg" />
                <p class="why-use-headline">
                  <strong>Handles geospatial datasets</strong>
                </p>
                <p class="xsmall-body text-lightGray">
                  Handles a variety of geospatial file formats, map-based
                  coordinates, incomplete labeling, and multiband (3+ bands)
                  imagery.
                </p>
              </div>
              <div>
                <img
                  class="rounded-lg"
                  src="images/why-use-massiveimgagery.jpg"
                />
                <p class="why-use-headline">
                  <strong>Supports massive imagery</strong>
                </p>
                <p class="xsmall-body text-lightGray">
                  Works out-of-the-box with massive imagery commonly used in the
                  geospatial field.
                </p>
              </div>
              <div>
                <img class="rounded-lg" src="images/why-use-lowbarrier.jpg" />
                <p class="why-use-headline">
                  <strong>Low barrier to entry</strong>
                </p>
                <p class="xsmall-body text-lightGray">
                  Enjoy a high-level programmatic API with sensible defaults for
                  configuring modeling pipelines. Doesn't require expertise in
                  PyTorch or deep learning.
                </p>
              </div>
              <div>
                <img class="rounded-lg" src="images/why-use-fastaws.jpg" />
                <p class="why-use-headline">
                  <strong>Fast AWS Batch setup process</strong>
                </p>
                <p class="xsmall-body text-lightGray">
                  Skip the manual setup and use CloudFormation templates.
                </p>
              </div>
              <div>
                <img class="rounded-lg" src="images/why-use-extensible.jpg" />
                <p class="why-use-headline">
                  <strong>Extensible architecture</strong>
                </p>
                <p class="xsmall-body text-lightGray">
                  Object-oriented architecture is extendable to new computer
                  vision tasks and deep learning frameworks.
                </p>
              </div>
            </div>
          </div>
        </div>
      </div>
    </section>
    <section>
      <div class="bg-white">
        <div class="max-width-container sm:mt-10">
          <h2 class="text-lg text-center mb-12 sm:mb-7 md:text-2xl">
            Easily use models in various deployment scenarios
          </h2>
          <div class="sm:flex sm:flex-row gap-x-4 lg:mx-12">
            <div
              class="mb-10 h-fit md:mb-0 rounded-lg sm:min-w-repeatable-config-text-narrow bg-repeatable-config-background p-6 sm:p-2 md:p-6 basis-1/3"
            >
              <h1 class="text-2xl md:text-3xl">
                Quick & repeatable configurations
              </h1>
              <h4 class="text-lightGray mt-6 mb-5">Select a scenario</h4>
              <div class="flex flex-col gap-3">
                <div
                  id="repeatable-config-0"
                  class="repeatable-config-selection selected border-2 border-teal-button"
                >
                  Semantic segmentation
                  <div class="repeatable-config-button-container">
                    <div class="repeatable-config-outer-button">
                      <div
                        id="repeatable-config-button-0"
                        class="repeatable-config-inner-button bg-teal-button"
                      ></div>
                    </div>
                  </div>
                </div>
                <div
                  id="repeatable-config-1"
                  class="repeatable-config-selection"
                >
                  Chip classification
                  <div class="repeatable-config-button-container">
                    <div class="repeatable-config-outer-button">
                      <div
                        id="repeatable-config-button-1"
                        class="repeatable-config-inner-button"
                      ></div>
                    </div>
                  </div>
                </div>
                <div
                  id="repeatable-config-2"
                  class="repeatable-config-selection"
                >
                  Object detection
                  <div class="repeatable-config-button-container">
                    <div class="repeatable-config-outer-button">
                      <div
                        id="repeatable-config-button-2"
                        class="repeatable-config-inner-button"
                      ></div>
                    </div>
                  </div>
                </div>
              </div>
              <p class="small-body text-lightGray mt-6">
                Based on open-source data.
              </p>
            </div>
            <div
              class="mb-20 max-h-repeatable-config-img max-w-repeatable-config-img"
            >
              <img
                id="legend"
                class="absolute"
                src="images/demo-segsem-legend.svg"
              />
              <img
                class="rounded-lg object-cover"
                id="repeatable-config-img"
                alt="A satellite image of a suburban neighborhood with buildings highlighted"
                src="images/demo-semseg.jpg"
              />
            </div>
          </div>
        </div>
      </div>
    </section>
    <section>
      <div class="bg-background-blue">
        <div class="max-width-container">
          <div class="sm:flex sm:flex-row-reverse">
            <div>
              <h1 class="text-white">How can we help?</h1>
              <p class="medium-body text-white">
                Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do
                eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut
                enim ad minim veniam, quis nostrud exercitation ullamco laboris
                nisi ut aliquip ex ea commodo consequat.
              </p>
            </div>
            <p class="medium-body text-white py-5">
              Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do
              eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut
              enim ad minim veniam, quis nostrud exercitation ullamco laboris
              nisi ut aliquip ex ea commodo consequat.
            </p>
          </div>
        </div>
      </div>
    </section>
    <section>
      <div class="bg-footer-blue">
        <div class="max-width-container">
          <h2 class="text-white text-center mt-4 sm:mt-8">
            An open-source computer vision and machine-learning framework
            developed by
            <a href="https://www.azavea.com/"
              ><span class="text-blue underline">Azavea</span></a
            >.
          </h2>
          <div
            class="flex flex-col-reverse sm:flex-row justify-center text-center xsmall-body text-white mt-16"
          >
            <div>©2022 Azavea Inc. All rights reserved.</div>
            <div class="mb-2 sm:mb-0">
              <a
                href="https://azavea.com/privacy-policy/"
                class="underline sm:ml-4"
                >Privacy Policy</a
              >
            </div>
          </div>
        </div>
      </div>
    </section>
    <script>
      typewriterEffect(
        document.getElementById("hero-text"),
        "measuring traffic density"
      );
    </script>
    <script>
      let interval = setInterval(changeHeroTextAndImage, 5000, true);
      $("#left-button").click(function () {
        clearInterval(interval);
        changeHeroTextAndImage(false);
        interval = setInterval(changeHeroTextAndImage, 5000, true);
      });
      $("#right-button").click(function () {
        clearInterval(interval);
        changeHeroTextAndImage(true);
        interval = setInterval(changeHeroTextAndImage, 5000, true);
      });
      //Add desktop header on scroll
      window.onscroll = function () {
        evalHeaderOnChange();
      };
    </script>
    <script>
      $("#repeatable-config-0").click(function () {
        changeRepeatableConfigImage(this);
      });
      $("#repeatable-config-1").click(function () {
        changeRepeatableConfigImage(this);
      });
      $("#repeatable-config-2").click(function () {
        changeRepeatableConfigImage(this);
      $("#spec-button").click(function () {
        const specSection = document.getElementById("spec-section");
        const specButton = document.getElementById("spec-button");
        if (specSection.classList.contains("hidden")) {
          specSection.classList.remove("hidden");
          specButton.innerHTML = "See less specs";
        } else {
          specSection.classList.add("hidden");
          specButton.innerHTML = "See more specs";
        }
      });
    </script>
  </body>
</html>
